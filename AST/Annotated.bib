%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End-to-end speech translation for Low-Resource Languages %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{Weiss:17,
	author={Weiss, Ron J. and Chorowski, Jan and Jaitly, Navdeep and Wu, Yonghui and Chen, Zhifeng},
	year={2017},
	title={Sequence-to-Sequence Models Can Directly Translate Foreign Speech},
	DOI={10.21437/interspeech.2017-503},
	journal={Interspeech 2017},
	annote={
		The authors present an encoder-decoder deep recurrent neural network for direct speech-to-text translation,
		without transcription into text in source language. A single attention-based sequence-to-sequence model is used
		for end-to-end speech translation, which the authors argue is more powerful than independent ASR and MT models,
		as indicated by the improved BLEU score. It is mentioned that this method is viable for low-resource settings,
		and can be augmented if speech in a low-resource language has text transcriptions in a high-resource language.
	}
}

@article{Bansal:19,
	author={Bansal, Sameer and Kamper, Herman and Livescu, Karen and Lopez, Adam and Goldwater, Sharon},
	year={2019},
	title={Pre-training on high-resource speech recognition improves low-resource speech-to-text translation},
	DOI={10.18653/v1/n19-1006},
	journal={Proceedings of the 2019 Conference of the North},
	annote={
		This paper investigates a method to improve direct speech-to-text translation (ST) for low resource languages.
		Since there is very sparse data available, instead of traditional ST (ASR and MT), the authors implement
		end-to-end ST by pre-training a model on a high resource language (English in this example) and fine-tuning the
		parameters for the low-resource language. BLEU Machine Translation scores are used as the metric to show
		improvement in results.
	}
}

@article{Bansal:18,
	author={Bansal, Sameer and Kamper, Herman and Livescu, Karen and Lopez, Adam and Goldwater, Sharon},
	year={2018},
	title={Low-Resource Speech-to-Text Translation},
	DOI={10.21437/interspeech.2018-1326},
	journal={Interspeech 2018},
	annote={
		This paper aims to investigate if the neural encoder-decoder model approach, that can directly translate speech
		to text in high-resource settings, will also work in low-resource settings (in terms of both data and compute
		power). The authors use word-level decoding instead of character-level to use fewer computational resources,
		which, along with the small amount of data, yields a lower BLEU score, but better precision and recall scores
		for word prediction.
	}
}

@article{Anastasopoulos:18,
	author={Anastasopoulos, Antonios and Chiang, David},
	year={2018},
	title={Tied Multitask Learning for Neural Speech Translation},
	DOI={10.18653/v1/n18-1008},
	journal={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	annote={
		This paper explores tied multi-task learning with sequence-to-sequence models, where the second task decoder
		receives input from both the encoder as well as the first task decoder. This information from a higher-level
		intermediate representation is intuitively useful, and is proved by the improved results. The authors also
		reason that transitivity and invertibility are intuitive principles, and add a regularization term to the
		model's attention mechanism so that it conforms to these principles.
	}
}

@article{Berard:18,
	author={B{\'e}rard, Alexandre and Besacier, Laurent and Kocabiyikoglu, Ali Can and Pietquin, Olivier},
	year={2018},
	title={End-to-End Automatic Speech Translation of Audiobooks},
	DOI={10.1109/icassp.2018.8461690},
	journal={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	annote={
		The authors of this paper investigate end-to-end speech translation in one single pass. The authors, however,
		make source language transcription available during training. An encoder-decoder model with attention is used
		on a corpus of audiobooks specifically augmented for this task. The authors propose their model as the baseline
		that performs almost as well as cascading two neural models for ASR and MT.
	}
}

@article{Deng:13,
	author={Deng, Li and Li, Jinyu and Huang, Jui-Ting and Yao, Kaisheng and Yu, Dong and Seide, Frank and Seltzer, Michael and Zweig, Geoff and He, Xiaodong and Williams, Jason and et al.},
	year={2013},
	title={Recent advances in deep learning for speech research at Microsoft},
	DOI={10.1109/icassp.2013.6639345},
	journal={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
	annote={
		This paper explores deep learning techniques for speech recognition and feature coding that are suitable
		replacements for earlier architecture that used Gaussians associated with HMM states. The authors apply these
		methods to various speech technology applications, and present improved results for many classic tasks. These
		techniques can be extended to other areas, end-to-end speech translation in particular.
	}
}

@article{Zoph:16,
	author={Zoph, Barret and Yuret, Deniz and May, Jonathan and Knight, Kevin},
	year={2016},
	title={Transfer Learning for Low-Resource Neural Machine Translation},
	DOI={10.18653/v1/d16-1163},
	journal={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	annote={
		The authors of this paper propose a transfer-learning approach by training a model on a pair of high-resource
		languages, and then transfer those parameters to the low-resource pair. This paper tackles the limitations of
		traditional neural MT networks in low-resource settings, which perform poorly in comparison to other MT systems.
		It is also concluded that a model based on a pair of high-resource language outperforms that based on a single
		language, and even the choice of languages affects performance.
	}
}

@article{Johnson:17,
	author={Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg and et al.},
	year={2017},
	title={Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	volume={5},
	DOI={10.1162/tacl_a_00065},
	journal={Transactions of the Association for Computational Linguistics},
	annote={
		This paper proposes a standard neural MT system for multilingual translation. The encoder, decoder and attention
		module are shared across all languages, and the model accepts a token that indicates the target language. Using
		a shared word-piece vocabulary and no increase in parameters, this approach surpasses state-of-the-art on WMT'14
		and WMT'15 benchmarks. The multilingual model allows for better translation, including transfer learning and
		zero-shot translation by implicit bridging between language pairs that were not seen during training.
	}
}

@article{Miao:13,
	author={Miao, Yajie and Metze, Florian and Rawat, Shourabh},
	year={2013},
	title={Deep maxout networks for low-resource speech recognition},
	DOI={10.1109/asru.2013.6707763},
	journal={2013 IEEE Workshop on Automatic Speech Recognition and Understanding},
	annote={
		This paper applies deep maxout networks architecture on low-resource languages with limited text transcriptions.
		The success of this model lies in the reduced hidden activations, that shrink the size of the parameters, which
		make it very suitable for low-resource settings. This introduction of sparsity in the hidden activations also
		make these models suitable sparse feature extractors.
	}
}

@article{Thomas:13,
	author={Thomas, Samuel and Seltzer, Michael L. and Church, Kenneth and Hermansky, Hynek},
	year={2013},
	title={Deep neural network features and semi-supervised training for low resource speech recognition},
	DOI={10.1109/icassp.2013.6638959},
	journal={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
	annote={
		The authors of this paper explore a new technique for training deep neural networks for large vocabulary
		continuous speech recognition in low-resource settings. Transcribed multilingual data and semi-supervised
		training are used to tackle the lack of sufficient data for acoustic modeling. Both these approaches show a
		great improvement in results with a very small amount of data.
	}
}

@article{Thomas:16,
	author={Thomas, Samuel and Audhkhasi, Kartik and Cui, Jia and Kingsbury, Brian and Ramabhadran, Bhuvana},
	year={2016},
	title={Multilingual Data Selection for Low Resource Speech Recognition},
	DOI={10.21437/interspeech.2016-598},
	journal={Interspeech 2016},
	annote={
		This paper introduces a technique for data selection that discovers language groups from a set of training
		languages. The authors recognize that feature representations extracted from deep neural network based
		multilingual frontends show improved results for speech recognition in low-resource settings. They argue that
		their data selection model can be used to effectively reduce training data and time significantly, without
		affecting performance. This approach is extremely useful for low-resource settings with only a few hours of
		data available.
	}
}

@article{Chen:15,
	author={Chen, Dongpeng and Mak, Brian},
	year={2015},
	title={Multi-task Learning of Deep Neural Networks for Low-resource Speech Recognition},
	DOI={10.1109/taslp.2015.2422573},
	journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	annote={
		The authors of this paper propose a deep neural network based multi-task learning approach for low-resource ASR.
		They show that training grapheme models in parallel improves the performance of phone models for a single
		language. While training multiple languages, the authors explore the learning of a set of Universal Phones to
		improve the phone models of all the languages. Using this approach on three low-resource languages shows a
		significant improvement in word-recognition gains.
	}
}